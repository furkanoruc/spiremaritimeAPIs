{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxTrqGxAH_M0"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "A Python script that:\n",
        "  - Queries Spire Maritime's GraphQL API using the query defined in GRAPHQL_QUERY.\n",
        "  - Paginates results (fetches up to 1000 per page) using internal pagination variables.\n",
        "  - Runs continuously at a configurable interval (default: 60 seconds) to update AIS data.\n",
        "  - Upserts new data into a CSV. Depending on configuration, it either:\n",
        "       * Replaces the old AIS record for a vessel (one row per vessel), or\n",
        "       * Saves historical messages by appending a new row if any field has changed.\n",
        "  - Verbose printing lets you follow the ingestion steps.\n",
        "\n",
        "IMPORTANT:\n",
        "  1. Replace 'YOUR_SPIRE_TOKEN' with your actual Bearer token from Spire.\n",
        "  2. Update the GRAPHQL_QUERY variable below if you wish to change filtering.\n",
        "     This is now the only userâ€“facing change.\n",
        "  3. The script stores data in a file named 'vessels_data.csv' by default.\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from IPython.display import display  # Optional, for Jupyter\n",
        "import os\n",
        "import time\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. USER CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "SPIRE_GRAPHQL_ENDPOINT = \"https://api.spire.com/graphql\"\n",
        "BEARER_TOKEN = \"your-token\"  # <-- REPLACE THIS\n",
        "CSV_FILENAME = \"vessels_data.csv\"\n",
        "\n",
        "# Run continuously? If False, the script runs once.\n",
        "RUN_CONTINUOUS = True\n",
        "# Interval between runs in seconds (default 60 seconds = 1 minute)\n",
        "RUN_INTERVAL = 60\n",
        "\n",
        "# Save historical messages? If True, new AIS data will be appended (only if data changed);\n",
        "# If False, the latest AIS data for a vessel will simply replace the old one.\n",
        "SAVE_HISTORY = False\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. THE GRAPHQL QUERY\n",
        "#\n",
        "#    NOTE: Update this query if you want to change your search/filtering.\n",
        "#    For example, to change the MMSI value or use IMO, update the query below.\n",
        "#    The pagination variables ($first and $after) are still supplied automatically.\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "GRAPHQL_QUERY = \"\"\"\n",
        "query GetVessels(\n",
        "  $first: Int,\n",
        "  $after: String\n",
        ") {\n",
        "  vessels(\n",
        "    #shipType: [LNG_CARRIER]  # <-- Change this value or modify to use imo instead.\n",
        "      areaOfInterest: { polygon: { type: \"Polygon\" coordinates: [ [ [-125.61293378057832,37.85152006848887],[-121.55390635761478,31.37524717787872],[-115.60295937310033,32.77427969380787],[-120.72834145802892,38.473576456397296],[-125.61293378057832,37.85152006848887] ] ] } }\n",
        "      lastPositionUpdate:{\n",
        "  startTime:\"2025-06-27T17:10:00.00Z\"\n",
        "}\n",
        "    first: $first,\n",
        "    after: $after\n",
        "  ) {\n",
        "    pageInfo {\n",
        "      endCursor\n",
        "      hasNextPage\n",
        "    }\n",
        "    totalCount{\n",
        "      relation\n",
        "      value\n",
        "    }\n",
        "    nodes {\n",
        "      staticData {\n",
        "        name\n",
        "        imo\n",
        "        mmsi\n",
        "        aisClass\n",
        "        callsign\n",
        "        flag\n",
        "        shipType\n",
        "        updateTimestamp\n",
        "        dimensions {\n",
        "          length\n",
        "          width\n",
        "        }\n",
        "        validated {\n",
        "          name\n",
        "          imo\n",
        "          callsign\n",
        "          shipType\n",
        "          dimensions {\n",
        "            length\n",
        "            width\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      lastPositionUpdate {\n",
        "        accuracy\n",
        "        collectionType\n",
        "        course\n",
        "        heading\n",
        "        latitude\n",
        "        longitude\n",
        "        maneuver\n",
        "        navigationalStatus\n",
        "        rot\n",
        "        speed\n",
        "        timestamp\n",
        "        updateTimestamp\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. HELPER FUNCTION TO FETCH (PAGINATE) DATA\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def fetch_vessel_data(token: str, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Fetch vessel data from Spire's GraphQL API in a paginated manner.\n",
        "    Uses the GRAPHQL_QUERY defined above and supplies pagination variables.\n",
        "    \"\"\"\n",
        "    all_nodes = []\n",
        "    variables = {\"first\": 1000, \"after\": None}\n",
        "    page_count = 0\n",
        "\n",
        "    while True:\n",
        "        page_count += 1\n",
        "        if verbose:\n",
        "            print(f\"\\n[fetch_vessel_data] Requesting page {page_count} ...\")\n",
        "        payload = {\n",
        "            \"query\": GRAPHQL_QUERY,\n",
        "            \"variables\": variables\n",
        "        }\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"Authorization\": f\"Bearer {token}\"\n",
        "        }\n",
        "        response = requests.post(\n",
        "            SPIRE_GRAPHQL_ENDPOINT,\n",
        "            json=payload,\n",
        "            headers=headers,\n",
        "            timeout=30\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if \"errors\" in data:\n",
        "            raise Exception(f\"GraphQL returned errors: {data['errors']}\")\n",
        "        vessels_data = data[\"data\"][\"vessels\"]\n",
        "        page_nodes = vessels_data[\"nodes\"]\n",
        "        all_nodes.extend(page_nodes)\n",
        "        has_next_page = vessels_data[\"pageInfo\"][\"hasNextPage\"]\n",
        "        end_cursor = vessels_data[\"pageInfo\"][\"endCursor\"]\n",
        "        if verbose:\n",
        "            print(f\" - Page {page_count} returned {len(page_nodes)} nodes.\")\n",
        "            print(f\" - hasNextPage = {has_next_page}, endCursor = {end_cursor}\")\n",
        "        if not has_next_page:\n",
        "            break\n",
        "        variables[\"after\"] = end_cursor\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[fetch_vessel_data] Finished. Total nodes fetched: {len(all_nodes)}\")\n",
        "    return all_nodes\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. FUNCTION TO UP-OR-INSERT THE RAW QUERY OUTPUT INTO CSV\n",
        "#\n",
        "#    This function:\n",
        "#      - Uses pd.json_normalize to flatten the JSON data,\n",
        "#      - Dynamically selects a primary key based on the query:\n",
        "#            * If 'staticData.mmsi' is present, it is used.\n",
        "#            * Otherwise, 'staticData.imo' is used.\n",
        "#      - Depending on SAVE_HISTORY:\n",
        "#            * If True: new rows are appended only if the AIS data for a vessel has changed.\n",
        "#            * If False: new data replaces the previous record for a vessel.\n",
        "#      - Writes the result to CSV.\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def upsert_into_csv(nodes: list, csv_filename: str, save_history: bool, verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Merge the list of nodes from Spire into an existing CSV (if it exists)\n",
        "    or create a new one. Dynamically handles columns based on the query output.\n",
        "\n",
        "    Primary key selection:\n",
        "      - If 'staticData.mmsi' exists, it is used.\n",
        "      - Otherwise, 'staticData.imo' is used.\n",
        "\n",
        "    In non-historical mode (save_history=False), rows for the same vessel (primary key)\n",
        "    are merged with the new data overriding the old.\n",
        "\n",
        "    In historical mode (save_history=True), new rows are appended only if the latest AIS data\n",
        "    for a vessel (compared to the last historical record) has changed.\n",
        "    \"\"\"\n",
        "    # Flatten the JSON data\n",
        "    df_new = pd.json_normalize(nodes)\n",
        "\n",
        "    if df_new.empty:\n",
        "        if verbose:\n",
        "            print(\"[upsert_into_csv] No new nodes to upsert.\")\n",
        "        return pd.read_csv(csv_filename) if os.path.exists(csv_filename) else df_new\n",
        "\n",
        "    # Determine primary key\n",
        "    primary_key = None\n",
        "    if \"staticData.mmsi\" in df_new.columns:\n",
        "        primary_key = \"staticData.mmsi\"\n",
        "    elif \"staticData.imo\" in df_new.columns:\n",
        "        primary_key = \"staticData.imo\"\n",
        "\n",
        "    if primary_key is None:\n",
        "        primary_key = \"staticData.mmsi\"\n",
        "        #raise ValueError(\"Neither 'staticData.mmsi' nor 'staticData.imo' found in query results for primary key.\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[upsert_into_csv] Using '{primary_key}' as the primary key for upsert.\")\n",
        "\n",
        "    # Read existing CSV if available\n",
        "    if os.path.isfile(csv_filename):\n",
        "        if verbose:\n",
        "            print(f\"[upsert_into_csv] Reading existing CSV: {csv_filename}\")\n",
        "        df_existing = pd.read_csv(csv_filename, dtype=str)\n",
        "    else:\n",
        "        if verbose:\n",
        "            print(f\"[upsert_into_csv] No existing CSV found. Creating new DataFrame.\")\n",
        "        df_existing = pd.DataFrame()\n",
        "\n",
        "    # Ensure the primary key is a string\n",
        "    df_new[primary_key] = df_new[primary_key].astype(str)\n",
        "    if not df_existing.empty and primary_key in df_existing.columns:\n",
        "        df_existing[primary_key] = df_existing[primary_key].astype(str)\n",
        "\n",
        "    if save_history:\n",
        "        # Historical mode: append new row only if there is no previous record for that vessel,\n",
        "        # or if the AIS data has changed compared to the last record.\n",
        "        if not df_existing.empty and primary_key in df_existing.columns:\n",
        "            # Get the last record for each vessel based on the primary key.\n",
        "            last_records = df_existing.sort_index().groupby(primary_key, as_index=False).last()\n",
        "        else:\n",
        "            last_records = pd.DataFrame()\n",
        "\n",
        "        rows_to_append = []\n",
        "        for _, new_row in df_new.iterrows():\n",
        "            key_val = new_row[primary_key]\n",
        "            if not last_records.empty and key_val in last_records[primary_key].values:\n",
        "                last_record = last_records[last_records[primary_key] == key_val].iloc[0]\n",
        "                # Compare common columns (if any differences, append new record)\n",
        "                common_cols = set(new_row.index).intersection(last_record.index)\n",
        "                if new_row[list(common_cols)].to_dict() != last_record[list(common_cols)].to_dict():\n",
        "                    rows_to_append.append(new_row)\n",
        "                else:\n",
        "                    if verbose:\n",
        "                        print(f\"[upsert_into_csv] No changes for {primary_key}={key_val}\")\n",
        "            else:\n",
        "                rows_to_append.append(new_row)\n",
        "        if rows_to_append:\n",
        "            df_append = pd.DataFrame(rows_to_append)\n",
        "            df_merged = pd.concat([df_existing, df_append], ignore_index=True)\n",
        "        else:\n",
        "            df_merged = df_existing.copy()\n",
        "        if verbose:\n",
        "            print(f\"[upsert_into_csv] Historical upsert complete. Total rows: {len(df_merged)}\")\n",
        "    else:\n",
        "        # Replacement mode: new data simply replaces old data based on primary key.\n",
        "        if not df_existing.empty:\n",
        "            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
        "            df_merged = df_combined.groupby(primary_key, as_index=False).last()\n",
        "        else:\n",
        "            df_merged = df_new\n",
        "        if verbose:\n",
        "            print(f\"[upsert_into_csv] Replacement upsert complete. Total rows: {len(df_merged)}\")\n",
        "\n",
        "    # Write merged DataFrame to CSV.\n",
        "    df_merged.to_csv(csv_filename, index=False)\n",
        "    return df_merged\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. MAIN ENTRY POINT\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def main(verbose: bool = True):\n",
        "    \"\"\"\n",
        "    Main routine:\n",
        "      1. Fetch data using the query defined in GRAPHQL_QUERY.\n",
        "      2. Upsert the data into the CSV based on the chosen mode.\n",
        "      3. Optionally display the top rows (for demonstration in Jupyter).\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"=== Starting script ===\")\n",
        "    nodes = fetch_vessel_data(token=BEARER_TOKEN, verbose=verbose)\n",
        "    df_merged = upsert_into_csv(nodes=nodes, csv_filename=CSV_FILENAME, save_history=SAVE_HISTORY, verbose=verbose)\n",
        "    if verbose:\n",
        "        print(f\"=== Final dataframe after upsert: {len(df_merged)} rows ===\")\n",
        "        display(df_merged.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 6. EXECUTION LOOP\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if RUN_CONTINUOUS:\n",
        "        while True:\n",
        "            main(verbose=True)\n",
        "            print(f\"Waiting {RUN_INTERVAL} seconds until next update...\\n\")\n",
        "            time.sleep(RUN_INTERVAL)\n",
        "    else:\n",
        "        main(verbose=True)"
      ],
      "metadata": {
        "id": "wgkO03_LIABN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}